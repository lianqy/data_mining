{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python实现logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Array, Lock, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lordData(path):\n",
    "    data = []\n",
    "    label = []\n",
    "    with open(path, 'r') as fr:\n",
    "        for line in fr.readlines():\n",
    "            index = line.find(' ')\n",
    "            data.append(line[index + 1 : ])\n",
    "            label.append(int(line[0]))\n",
    "    return data, label\n",
    "\n",
    "def parseData(data):\n",
    "    dataDictionary = {}\n",
    "    features = data.strip().split() #先去除换行符等，而后根据空格切片\n",
    "    for item in features:\n",
    "        index,fea = item.split(':') #解析每个index的特征\n",
    "        dataDictionary[int(index)] = fea\n",
    "        #print dataDictionary\n",
    "    return dataDictionary  #返回稀疏特征\n",
    "\n",
    "def normalization(dataDict):\n",
    "    #对数据进行归一化\n",
    "    data_sum = sum(float(data) for i,data in dataDict.items())\n",
    "    data_sum_sq = sum(float(data)*float(data) for i,data in dataDict.items())\n",
    "    data_mean = float(data_sum / 201)  #均值\n",
    "    data_mean_sq = float(data_sum_sq / 201) #平方的均值\n",
    "    data_var = data_mean_sq - data_mean * data_mean #方差\n",
    "    data_std = math.sqrt(data_var)\n",
    "    #print data_mean\n",
    "    #print data_mean_sq\n",
    "    #print data_std\n",
    "    result = {}\n",
    "    for i in range(202):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if dataDict.has_key(i):\n",
    "            result[i] = float((float(dataDict[i]) - data_mean) / data_std)\n",
    "        else:\n",
    "            result[i] = float((0 - data_mean) / data_std)\n",
    "    return result\n",
    "    \n",
    "def sigmoid(x):\n",
    "    if x > 30:\n",
    "        return 1.0\n",
    "    if x < -30:\n",
    "        return 0.0\n",
    "    return 1.0/(1 + math.exp(-x))\n",
    "\n",
    "def predict(weights, dataDict):\n",
    "    z = 0.0\n",
    "    for index in dataDict:\n",
    "        z = z + weights[index] * dataDict[index]\n",
    "    z = z + weights[0] #加上常数项\n",
    "    return z\n",
    "\n",
    "def calAcc(weights, val_data, val_label, iter):\n",
    "    hit = 0\n",
    "    num = len(val_data)\n",
    "    for i in range(num):\n",
    "        dataDict = val_data[i]\n",
    "        z = predict(weights, dataDict)\n",
    "        if ((z >= 0) and val_label[i] == 1) or ((z < 0) and val_label[i] == 0):\n",
    "            hit = hit + 1\n",
    "        acc = float(hit) / num\n",
    "    print 'iter', iter, ': accuracy = ', hit, '/', num, '=', acc\n",
    "        \n",
    "    \n",
    "def LogisticRegression(tdata, tlabel, val_data, val_label, param = {'learning_rate':0.01, 'batch_size':1000, 'max_iter':1000}):\n",
    "    #获取参数学习率，若无设置，默认为0.01\n",
    "    if param.has_key('learning_rate'):\n",
    "        learning_rate = param['learning_rate']\n",
    "    else:\n",
    "        learning_rate = 0.01\n",
    "    #获取参数batchsize，若无设置，默认为1000\n",
    "    if param.has_key('batch_size'):\n",
    "        batch = param['batch_size']\n",
    "    else:\n",
    "        batch = 1000\n",
    "    #获取参数最大迭代数，若无设置，默认为1000\n",
    "    if param.has_key('max_iter'):\n",
    "        max_iter = param['max_iter']\n",
    "    else:\n",
    "        max_iter = 1000\n",
    "    \n",
    "    #获取特征的系数，共有201个特征，加上常数项，因此有202个需要学习的系数\n",
    "    #这里采用随机初始化为[0,1)间的值\n",
    "    weights = [random.random() for i in range(202)]\n",
    "    totalTime = 0.0\n",
    "    #通过minibatch随机梯度下降进行系数的更新\n",
    "    for iter in range(max_iter):\n",
    "        start = time.time()\n",
    "        #rindex = int(random.uniform(0, len(tdata) - batch))\n",
    "        rindex = int(0 + iter * batch)\n",
    "        while (rindex >= len(tdata)):\n",
    "            rindex = rindex - len(tdata)\n",
    "        #weights_temp = [weights[i] for i in range(202)]\n",
    "        error = np.zeros(201)\n",
    "        loss = 0.0\n",
    "        for i in range(batch):\n",
    "            dataDict = tdata[rindex + i]\n",
    "            if (tlabel[rindex + i] == 1):\n",
    "                loss = loss - math.log(sigmoid(predict(weights, dataDict)))\n",
    "            else:\n",
    "                loss = loss - math.log(1 - sigmoid(predict(weights, dataDict))) \n",
    "            e_temp = sigmoid(predict(weights, dataDict)) - tlabel[rindex + i] \n",
    "            for index in dataDict:\n",
    "                error[index - 1] = error[index - 1] + e_temp * dataDict[index]\n",
    "        #计算loss\n",
    "        loss = loss / batch\n",
    "        print 'loss:',loss\n",
    "        for index in dataDict:\n",
    "            weights[index] = weights[index] - learning_rate * (float(1)/batch) * error[index - 1]\n",
    "        end = time.time()\n",
    "        totalTime = totalTime + float(end - start)\n",
    "        if ((iter + 1) % 10 == 0):\n",
    "            calAcc(weights, val_data, val_label, iter + 1)\n",
    "    print 'iter cost:', float(totalTime) / max_iter, 's in average.'\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdata, tlabel = lordData(\"./train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = parseData(tdata[0])\n",
    "t = normalization(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(100000):\n",
    "    temp1 = parseData(tdata[i])\n",
    "    temp2 = normalization(temp1)\n",
    "    train_data.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.71067992121\n",
      "loss: 1.63632032552\n",
      "loss: 1.55157230346\n",
      "loss: 1.48540930206\n",
      "loss: 1.48630348971\n",
      "loss: 1.30982765716\n",
      "loss: 1.2477162819\n",
      "loss: 1.18068695413\n",
      "loss: 1.12925550802\n",
      "loss: 1.14982796815\n",
      "loss: 1.10711832589\n",
      "loss: 1.06516425072\n",
      "loss: 1.05120964256\n",
      "loss: 1.02281088489\n",
      "loss: 1.02205683334\n",
      "loss: 0.973219044153\n",
      "loss: 0.964416915587\n",
      "loss: 0.954565216854\n",
      "loss: 0.923673390111\n",
      "loss: 0.93291751954\n",
      "loss: 0.866751145971\n",
      "loss: 0.888044014319\n",
      "loss: 0.798085451823\n",
      "loss: 0.848373541407\n",
      "loss: 0.851887309743\n",
      "loss: 0.793308528491\n",
      "loss: 0.847830500216\n",
      "loss: 0.811557383334\n",
      "loss: 0.754539282711\n",
      "loss: 0.772566531104\n",
      "loss: 0.736589920222\n",
      "loss: 0.770809051595\n",
      "loss: 0.764995125496\n",
      "loss: 0.750135394852\n",
      "loss: 0.747155032633\n",
      "loss: 0.726490136932\n",
      "loss: 0.708239486074\n",
      "loss: 0.703318005474\n",
      "loss: 0.661785965625\n",
      "loss: 0.706274630568\n",
      "loss: 0.662052185434\n",
      "loss: 0.681580191999\n",
      "loss: 0.672145707115\n",
      "loss: 0.647304750962\n",
      "loss: 0.668906591981\n",
      "loss: 0.669552569043\n",
      "loss: 0.629549315485\n",
      "loss: 0.632892881467\n",
      "loss: 0.661752611676\n",
      "loss: 0.642490663118\n",
      "loss: 0.676936947843\n",
      "loss: 0.637563783005\n",
      "loss: 0.653104948738\n",
      "loss: 0.6521659003\n",
      "loss: 0.61568552028\n",
      "loss: 0.629941394675\n",
      "loss: 0.642738523216\n",
      "loss: 0.642127236751\n",
      "loss: 0.615215931597\n",
      "loss: 0.637328462596\n",
      "loss: 0.6160852421\n",
      "loss: 0.593763777484\n",
      "loss: 0.637880951706\n",
      "loss: 0.626871979395\n",
      "loss: 0.59385968563\n",
      "loss: 0.645078007577\n",
      "loss: 0.607991214953\n",
      "loss: 0.61110528088\n",
      "loss: 0.575875000449\n",
      "loss: 0.613682309268\n",
      "loss: 0.630278385109\n",
      "loss: 0.586827453936\n",
      "loss: 0.629144001095\n",
      "loss: 0.60599213234\n",
      "loss: 0.580504960413\n",
      "loss: 0.601249194285\n",
      "loss: 0.605597684102\n",
      "loss: 0.61979928086\n",
      "loss: 0.626002713232\n",
      "loss: 0.591749548684\n",
      "loss: 0.61117251458\n",
      "loss: 0.615388700207\n",
      "loss: 0.623562678542\n",
      "loss: 0.586367013326\n",
      "loss: 0.603280470826\n",
      "loss: 0.572199786464\n",
      "loss: 0.609796503732\n",
      "loss: 0.59740139541\n",
      "loss: 0.603954558543\n",
      "loss: 0.619983581533\n",
      "loss: 0.618054746359\n",
      "loss: 0.586719769888\n",
      "loss: 0.626697139102\n",
      "loss: 0.642133454997\n",
      "loss: 0.575322202828\n",
      "loss: 0.605202858466\n",
      "loss: 0.590126720967\n",
      "loss: 0.597751840389\n",
      "loss: 0.5900086454\n",
      "loss: 0.596352977635\n",
      "iter 100 : accuracy =  677 / 1000 = 0.677\n",
      "loss: 0.602567302122\n",
      "loss: 0.613140728319\n",
      "loss: 0.541006258573\n",
      "loss: 0.620566701681\n",
      "loss: 0.585853938679\n",
      "loss: 0.573643798935\n",
      "loss: 0.596955280559\n",
      "loss: 0.604182827575\n",
      "loss: 0.591493928859\n",
      "loss: 0.567286761539\n",
      "loss: 0.607931830187\n",
      "loss: 0.589507166242\n",
      "loss: 0.589258157351\n",
      "loss: 0.603241888422\n",
      "loss: 0.579052001084\n",
      "loss: 0.58564486824\n",
      "loss: 0.571547525917\n",
      "loss: 0.60363511644\n",
      "loss: 0.606831716921\n",
      "loss: 0.616999107111\n",
      "loss: 0.568769866123\n",
      "loss: 0.622492981858\n",
      "loss: 0.594903054575\n",
      "loss: 0.572993409922\n",
      "loss: 0.590104922708\n",
      "loss: 0.557288479019\n",
      "loss: 0.599117720772\n",
      "loss: 0.630206056263\n",
      "loss: 0.562253591057\n",
      "loss: 0.582565183484\n",
      "loss: 0.57708900522\n",
      "loss: 0.598966975966\n",
      "loss: 0.617841798265\n",
      "loss: 0.59946661971\n",
      "loss: 0.603748055648\n",
      "loss: 0.596613683706\n",
      "loss: 0.570163754552\n",
      "loss: 0.585811851671\n",
      "loss: 0.542720658113\n",
      "loss: 0.600311528296\n",
      "loss: 0.550664989679\n",
      "loss: 0.572152055406\n",
      "loss: 0.57090218124\n",
      "loss: 0.551442931958\n",
      "loss: 0.603240423808\n",
      "loss: 0.586119290346\n",
      "loss: 0.560542431915\n",
      "loss: 0.557385702471\n",
      "loss: 0.59370475377\n",
      "loss: 0.579004740659\n",
      "loss: 0.600887238534\n",
      "loss: 0.569549311858\n",
      "loss: 0.596934753343\n",
      "loss: 0.59573354936\n",
      "loss: 0.562395067919\n",
      "loss: 0.563687353584\n",
      "loss: 0.591984186656\n",
      "loss: 0.593417483944\n",
      "loss: 0.570579839057\n",
      "loss: 0.58756225832\n",
      "loss: 0.568670419063\n",
      "loss: 0.554951670666\n",
      "loss: 0.594047509992\n",
      "loss: 0.58148114643\n",
      "loss: 0.564193022703\n",
      "loss: 0.609226925997\n",
      "loss: 0.568275524836\n",
      "loss: 0.583275973878\n",
      "loss: 0.549895576984\n",
      "loss: 0.58755140526\n",
      "loss: 0.604177388661\n",
      "loss: 0.559464232393\n",
      "loss: 0.598723165942\n",
      "loss: 0.576119173388\n",
      "loss: 0.556985331554\n",
      "loss: 0.581562358238\n",
      "loss: 0.585488362481\n",
      "loss: 0.604486885524\n",
      "loss: 0.602277265568\n",
      "loss: 0.574761939119\n",
      "loss: 0.598037431594\n",
      "loss: 0.600883732945\n",
      "loss: 0.596101423612\n",
      "loss: 0.568203107712\n",
      "loss: 0.584130834675\n",
      "loss: 0.548625394862\n",
      "loss: 0.597050201984\n",
      "loss: 0.579300937724\n",
      "loss: 0.589397586531\n",
      "loss: 0.601742198977\n",
      "loss: 0.608242817716\n",
      "loss: 0.567766216566\n",
      "loss: 0.612125141627\n",
      "loss: 0.628163973908\n",
      "loss: 0.558222889076\n",
      "loss: 0.590710716073\n",
      "loss: 0.581114872282\n",
      "loss: 0.585598508091\n",
      "loss: 0.582098518868\n",
      "loss: 0.584384379877\n",
      "iter 200 : accuracy =  683 / 1000 = 0.683\n",
      "loss: 0.594456278283\n",
      "loss: 0.596287022684\n",
      "loss: 0.529574007407\n",
      "loss: 0.607162761374\n",
      "loss: 0.571764181222\n",
      "loss: 0.563140915869\n",
      "loss: 0.587822920746\n",
      "loss: 0.597908186551\n",
      "loss: 0.587647881023\n",
      "loss: 0.560213116347\n",
      "loss: 0.596259578304\n",
      "loss: 0.579941754674\n",
      "loss: 0.581993870872\n",
      "loss: 0.594716155335\n",
      "loss: 0.566320867693\n",
      "loss: 0.576042934242\n",
      "loss: 0.562069164873\n",
      "loss: 0.593904859422\n",
      "loss: 0.598715203887\n",
      "loss: 0.608529838429\n",
      "loss: 0.561350109137\n",
      "loss: 0.613795121123\n",
      "loss: 0.59121678891\n",
      "loss: 0.563555525791\n",
      "loss: 0.58087480666\n",
      "loss: 0.550194402716\n",
      "loss: 0.586935714243\n",
      "loss: 0.623332788307\n",
      "loss: 0.554099951241\n",
      "loss: 0.576585078827\n",
      "loss: 0.571507704956\n",
      "loss: 0.591615652803\n",
      "loss: 0.610298780448\n",
      "loss: 0.592648777019\n",
      "loss: 0.597108865442\n",
      "loss: 0.591616623013\n",
      "loss: 0.562031239623\n",
      "loss: 0.57900576234\n",
      "loss: 0.53609072706\n",
      "loss: 0.596208154485\n",
      "loss: 0.546107954563\n",
      "loss: 0.567100268903\n",
      "loss: 0.564952409298\n",
      "loss: 0.545540280023\n",
      "loss: 0.59814521668\n",
      "loss: 0.580986463186\n",
      "loss: 0.554452480452\n",
      "loss: 0.552699355187\n",
      "loss: 0.589678051122\n",
      "loss: 0.57235557006\n",
      "loss: 0.592197198077\n",
      "loss: 0.562053972653\n",
      "loss: 0.592062137141\n",
      "loss: 0.591544378686\n",
      "loss: 0.557482181614\n",
      "loss: 0.555653747408\n",
      "loss: 0.586980243921\n",
      "loss: 0.587222927561\n",
      "loss: 0.566891631032\n",
      "loss: 0.579533432027\n",
      "loss: 0.563610656584\n",
      "loss: 0.551957617947\n",
      "loss: 0.587870325591\n",
      "loss: 0.575638242733\n",
      "loss: 0.560024490265\n",
      "loss: 0.603446349012\n",
      "loss: 0.562050230521\n",
      "loss: 0.578024017066\n",
      "loss: 0.547313736921\n",
      "loss: 0.583835350603\n",
      "loss: 0.600399070887\n",
      "loss: 0.555219908165\n",
      "loss: 0.592705897962\n",
      "loss: 0.568914882509\n",
      "loss: 0.553314307989\n",
      "loss: 0.577501187699\n",
      "loss: 0.580098495218\n",
      "loss: 0.60143693705\n",
      "loss: 0.597361823317\n",
      "loss: 0.571925043391\n",
      "loss: 0.593644480659\n",
      "loss: 0.596084700187\n",
      "loss: 0.58837974142\n",
      "loss: 0.563550850596\n",
      "loss: 0.580072788238\n",
      "loss: 0.541367135867\n",
      "loss: 0.591742271922\n",
      "loss: 0.57297255531\n",
      "loss: 0.58424009356\n",
      "loss: 0.596326768879\n",
      "loss: 0.603158123462\n",
      "loss: 0.561679167146\n",
      "loss: 0.605973183247\n",
      "loss: 0.621114564873\n",
      "loss: 0.553549604146\n",
      "loss: 0.586173456565\n",
      "loss: 0.577700246561\n",
      "loss: 0.581459990959\n",
      "loss: 0.578525982688\n",
      "loss: 0.579709525516\n",
      "iter 300 : accuracy =  686 / 1000 = 0.686\n",
      "loss: 0.590411217816\n",
      "loss: 0.588430359797\n",
      "loss: 0.527205275781\n",
      "loss: 0.600707408924\n",
      "loss: 0.567986447626\n",
      "loss: 0.558771765596\n",
      "loss: 0.583276684144\n",
      "loss: 0.594976397321\n",
      "loss: 0.584782947203\n",
      "loss: 0.557702061217\n",
      "loss: 0.589701968484\n",
      "loss: 0.576429181893\n",
      "loss: 0.578512041717\n",
      "loss: 0.589499517168\n",
      "loss: 0.561739023852\n",
      "loss: 0.570779929945\n",
      "loss: 0.558542374427\n",
      "loss: 0.588959153433\n",
      "loss: 0.594197540407\n",
      "loss: 0.603556637429\n",
      "loss: 0.55828280785\n",
      "loss: 0.608148070761\n",
      "loss: 0.588786803756\n",
      "loss: 0.559463539594\n",
      "loss: 0.575807241333\n",
      "loss: 0.547860001196\n",
      "loss: 0.580392152245\n",
      "loss: 0.617856734032\n",
      "loss: 0.550071115792\n",
      "loss: 0.573567964693\n",
      "loss: 0.568786318058\n",
      "loss: 0.587435895586\n",
      "loss: 0.605084853165\n",
      "loss: 0.587684295317\n",
      "loss: 0.592599568137\n",
      "loss: 0.588188768813\n",
      "loss: 0.557635968\n",
      "loss: 0.574957330168\n",
      "loss: 0.533147816419\n",
      "loss: 0.593561406937\n",
      "loss: 0.543878962846\n",
      "loss: 0.564493595211\n",
      "loss: 0.561756703494\n",
      "loss: 0.542900367442\n",
      "loss: 0.594086247098\n",
      "loss: 0.577778093777\n",
      "loss: 0.550193236201\n",
      "loss: 0.549901927212\n",
      "loss: 0.586594748148\n",
      "loss: 0.568092002308\n",
      "loss: 0.586528874027\n",
      "loss: 0.557102161049\n",
      "loss: 0.588946870171\n",
      "loss: 0.58884926002\n",
      "loss: 0.554319276408\n",
      "loss: 0.551065726337\n",
      "loss: 0.583562328173\n",
      "loss: 0.58239972683\n",
      "loss: 0.564749321813\n",
      "loss: 0.57408180824\n",
      "loss: 0.560161111778\n",
      "loss: 0.550410831195\n",
      "loss: 0.583614522777\n",
      "loss: 0.572424615837\n",
      "loss: 0.556553095489\n",
      "loss: 0.599204704817\n",
      "loss: 0.558244021795\n",
      "loss: 0.573668745935\n",
      "loss: 0.545626096757\n",
      "loss: 0.581089121391\n",
      "loss: 0.597575157764\n",
      "loss: 0.552759634233\n",
      "loss: 0.587601042898\n",
      "loss: 0.563536681172\n",
      "loss: 0.551075712676\n",
      "loss: 0.573887892033\n",
      "loss: 0.575637808233\n",
      "loss: 0.598547706645\n",
      "loss: 0.593818792846\n",
      "loss: 0.569730205064\n",
      "loss: 0.58961611758\n",
      "loss: 0.591848596523\n",
      "loss: 0.583257664337\n",
      "loss: 0.560094946676\n",
      "loss: 0.577239715907\n",
      "loss: 0.536395643804\n",
      "loss: 0.586729716941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.567900315149\n",
      "loss: 0.579932034508\n",
      "loss: 0.592152478494\n",
      "loss: 0.598192112055\n",
      "loss: 0.557148776955\n",
      "loss: 0.600866694948\n",
      "loss: 0.614875790894\n",
      "loss: 0.55045059924\n",
      "loss: 0.582639713547\n",
      "loss: 0.574811113987\n",
      "loss: 0.578044187149\n",
      "loss: 0.575428122311\n",
      "loss: 0.5760894501\n",
      "iter 400 : accuracy =  686 / 1000 = 0.686\n",
      "loss: 0.586736323096\n",
      "loss: 0.582069746165\n",
      "loss: 0.525878314508\n",
      "loss: 0.595156214479\n",
      "loss: 0.565568749987\n",
      "loss: 0.555255224663\n",
      "loss: 0.579614426961\n",
      "loss: 0.59229745512\n",
      "loss: 0.581751704539\n",
      "loss: 0.555925354276\n",
      "loss: 0.584145446183\n",
      "loss: 0.573672236316\n",
      "loss: 0.575355035581\n",
      "loss: 0.58466410873\n",
      "loss: 0.558591722287\n",
      "loss: 0.566219906168\n",
      "loss: 0.555937066601\n",
      "loss: 0.584797400575\n",
      "loss: 0.59027678261\n",
      "loss: 0.599098790381\n",
      "loss: 0.555907369091\n",
      "loss: 0.603361250601\n",
      "loss: 0.586606682531\n",
      "loss: 0.556314217904\n",
      "loss: 0.571462349235\n",
      "loss: 0.546312010205\n",
      "loss: 0.575182294476\n",
      "loss: 0.612683442943\n",
      "loss: 0.546985580078\n",
      "loss: 0.570972443189\n",
      "loss: 0.566666913742\n",
      "loss: 0.583943547863\n",
      "loss: 0.600482029488\n",
      "loss: 0.583069927834\n",
      "loss: 0.588484746308\n",
      "loss: 0.584988904995\n",
      "loss: 0.554158930739\n",
      "loss: 0.571620671111\n",
      "loss: 0.531159131983\n",
      "loss: 0.591145703441\n",
      "loss: 0.542088696547\n",
      "loss: 0.562200906392\n",
      "loss: 0.55912639864\n",
      "loss: 0.540989314372\n",
      "loss: 0.590610180069\n",
      "loss: 0.575030044734\n",
      "loss: 0.546624628747\n",
      "loss: 0.547574426609\n",
      "loss: 0.583656714957\n",
      "loss: 0.564506060608\n",
      "loss: 0.581711028732\n",
      "loss: 0.55290975434\n",
      "loss: 0.586235368156\n",
      "loss: 0.586585478894\n",
      "loss: 0.551749429662\n",
      "loss: 0.547412950213\n",
      "loss: 0.580620111114\n",
      "loss: 0.578134179492\n",
      "loss: 0.56296465541\n",
      "loss: 0.569581243864\n",
      "loss: 0.557070798845\n",
      "loss: 0.549191423954\n",
      "loss: 0.579960893481\n",
      "loss: 0.569881978045\n",
      "loss: 0.553326249871\n",
      "loss: 0.595542665463\n",
      "loss: 0.555156285034\n",
      "loss: 0.569711330945\n",
      "loss: 0.544142551336\n",
      "loss: 0.578776765001\n",
      "loss: 0.595170075887\n",
      "loss: 0.550858135056\n",
      "loss: 0.582883529397\n",
      "loss: 0.558863281676\n",
      "loss: 0.549270614453\n",
      "loss: 0.570563865229\n",
      "loss: 0.571728672549\n",
      "loss: 0.595907482865\n",
      "loss: 0.590765542233\n",
      "loss: 0.567743742523\n",
      "loss: 0.585983553379\n",
      "loss: 0.587989784026\n",
      "loss: 0.57911088555\n",
      "loss: 0.557160472069\n",
      "loss: 0.574856567079\n",
      "loss: 0.532348320261\n",
      "loss: 0.582064938066\n",
      "loss: 0.563533144633\n",
      "loss: 0.576154375268\n",
      "loss: 0.588502408119\n",
      "loss: 0.593563686432\n",
      "loss: 0.55328786581\n",
      "loss: 0.596332851498\n",
      "loss: 0.60935088022\n",
      "loss: 0.547869167245\n",
      "loss: 0.579626816604\n",
      "loss: 0.572234280261\n",
      "loss: 0.575013294825\n",
      "loss: 0.572745983615\n",
      "loss: 0.573023678403\n",
      "iter 500 : accuracy =  686 / 1000 = 0.686\n",
      "loss: 0.583536008044\n",
      "loss: 0.576560846121\n",
      "loss: 0.524889068003\n",
      "loss: 0.590238431206\n",
      "loss: 0.563607706135\n",
      "loss: 0.552264461092\n",
      "loss: 0.576556348364\n",
      "loss: 0.589852176957\n",
      "loss: 0.578884848135\n",
      "loss: 0.554554454218\n",
      "loss: 0.579317589046\n",
      "loss: 0.571284121019\n",
      "loss: 0.572463528349\n",
      "loss: 0.580302548387\n",
      "loss: 0.556130401025\n",
      "loss: 0.562242305503\n",
      "loss: 0.553817965831\n",
      "loss: 0.581142179644\n",
      "loss: 0.586797148318\n",
      "loss: 0.595102948252\n",
      "loss: 0.553907143985\n",
      "loss: 0.599273563817\n",
      "loss: 0.584731135034\n",
      "loss: 0.553637776651\n",
      "loss: 0.567616904493\n",
      "loss: 0.545103178287\n",
      "loss: 0.570758435817\n",
      "loss: 0.607987421823\n",
      "loss: 0.544418577145\n",
      "loss: 0.568675645799\n",
      "loss: 0.564923890237\n",
      "loss: 0.580980104021\n",
      "loss: 0.596416856537\n",
      "loss: 0.578889261885\n",
      "loss: 0.58481772726\n",
      "loss: 0.582109793026\n",
      "loss: 0.551223863008\n",
      "loss: 0.568739539037\n",
      "loss: 0.529663913465\n",
      "loss: 0.588987395779\n",
      "loss: 0.540592606833\n",
      "loss: 0.560152347552\n",
      "loss: 0.556835398198\n",
      "loss: 0.539443583495\n",
      "loss: 0.587691036808\n",
      "loss: 0.572667813282\n",
      "loss: 0.54361403884\n",
      "loss: 0.545602628521\n",
      "loss: 0.580980221703\n",
      "loss: 0.561395316072\n",
      "loss: 0.577485286171\n",
      "loss: 0.549270464844\n",
      "loss: 0.583872882946\n",
      "loss: 0.584709782433\n",
      "loss: 0.549666498927\n",
      "loss: 0.544328290852\n",
      "loss: 0.578092260967\n",
      "loss: 0.574385263899\n",
      "loss: 0.561455456598\n",
      "loss: 0.565790262679\n",
      "loss: 0.554304030251\n",
      "loss: 0.548189862142\n",
      "loss: 0.576791450884\n",
      "loss: 0.56772734229\n",
      "loss: 0.550426042797\n",
      "loss: 0.592393280765\n",
      "loss: 0.552524147016\n",
      "loss: 0.566179895099\n",
      "loss: 0.542851339024\n",
      "loss: 0.576861131261\n",
      "loss: 0.593153378949\n",
      "loss: 0.549338166682\n",
      "loss: 0.578647928205\n",
      "loss: 0.554784895165\n",
      "loss: 0.547791174844\n",
      "loss: 0.567635054816\n",
      "loss: 0.568362993471\n",
      "loss: 0.593624832496\n",
      "loss: 0.588103763045\n",
      "loss: 0.565987774159\n",
      "loss: 0.582813169342\n",
      "loss: 0.584577362167\n",
      "loss: 0.575613663029\n",
      "loss: 0.554673009698\n",
      "loss: 0.572827132822\n",
      "loss: 0.528906679245\n",
      "loss: 0.577886446463\n",
      "loss: 0.559777459629\n",
      "loss: 0.572900987794\n",
      "loss: 0.585299343718\n",
      "loss: 0.589426899722\n",
      "loss: 0.549954523181\n",
      "loss: 0.592354031489\n",
      "loss: 0.604597781374\n",
      "loss: 0.545642821279\n",
      "loss: 0.577058283627\n",
      "loss: 0.569985555366\n",
      "loss: 0.572361693861\n",
      "loss: 0.57049674649\n",
      "loss: 0.570417882393\n",
      "iter 600 : accuracy =  688 / 1000 = 0.688\n",
      "loss: 0.580860744544\n",
      "loss: 0.571800268244\n",
      "loss: 0.524124874527\n",
      "loss: 0.585975305191\n",
      "loss: 0.561927915947\n",
      "loss: 0.549753132224\n",
      "loss: 0.57401113008\n",
      "loss: 0.587702608647\n",
      "loss: 0.576340768595\n",
      "loss: 0.553482203412\n",
      "loss: 0.575158555782\n",
      "loss: 0.569227410035\n",
      "loss: 0.569907942039\n",
      "loss: 0.576541716183\n",
      "loss: 0.554189355919\n",
      "loss: 0.55886566091\n",
      "loss: 0.552098420467\n",
      "loss: 0.57798671637\n",
      "loss: 0.583769763583\n",
      "loss: 0.591634068563\n",
      "loss: 0.552229617739\n",
      "loss: 0.595822228345\n",
      "loss: 0.583193995328\n",
      "loss: 0.551333136\n",
      "loss: 0.56425638193\n",
      "loss: 0.544141483829\n",
      "loss: 0.566999878896\n",
      "loss: 0.603881348823\n",
      "loss: 0.542264701433\n",
      "loss: 0.566691206389\n",
      "loss: 0.563491250158\n",
      "loss: 0.578526628043\n",
      "loss: 0.592891778473\n",
      "loss: 0.575229893813\n",
      "loss: 0.581686039095\n",
      "loss: 0.579634876556\n",
      "loss: 0.548742394249\n",
      "loss: 0.566250209967\n",
      "loss: 0.528507479625\n",
      "loss: 0.587131400648\n",
      "loss: 0.539365329506\n",
      "loss: 0.558387652696\n",
      "loss: 0.554850867941\n",
      "loss: 0.538169573825\n",
      "loss: 0.585285001942\n",
      "loss: 0.570700035215\n",
      "loss: 0.541110222117\n",
      "loss: 0.54396281673\n",
      "loss: 0.578641255127\n",
      "loss: 0.558730821262\n",
      "loss: 0.573819716945\n",
      "loss: 0.546151499005\n",
      "loss: 0.581883253482\n",
      "loss: 0.583212029154\n",
      "loss: 0.548048206375\n",
      "loss: 0.541735986726\n",
      "loss: 0.575976827836\n",
      "loss: 0.571148351535\n",
      "loss: 0.560212540006\n",
      "loss: 0.562647244175\n",
      "loss: 0.55189795357\n",
      "loss: 0.547376832581\n",
      "loss: 0.574098473519\n",
      "loss: 0.565899341118\n",
      "loss: 0.547905965923\n",
      "loss: 0.589757001349\n",
      "loss: 0.550288735115\n",
      "loss: 0.563103862764\n",
      "loss: 0.54176412541\n",
      "loss: 0.575324503397\n",
      "loss: 0.591510304326\n",
      "loss: 0.548152215834\n",
      "loss: 0.574959804021\n",
      "loss: 0.551306859894\n",
      "loss: 0.546622304302\n",
      "loss: 0.565154117093\n",
      "loss: 0.565539072191\n",
      "loss: 0.591730348187\n",
      "loss: 0.585819888785\n",
      "loss: 0.564483968731\n",
      "loss: 0.580121683108\n",
      "loss: 0.581646591557\n",
      "loss: 0.572673226403\n",
      "loss: 0.552611137221\n",
      "loss: 0.571124565295\n",
      "loss: 0.52596535945\n",
      "loss: 0.574256952544\n",
      "loss: 0.556601010487\n",
      "loss: 0.57017136119\n",
      "loss: 0.582536582137\n",
      "loss: 0.585848117331\n",
      "loss: 0.547115270746\n",
      "loss: 0.588948125918\n",
      "loss: 0.600634334572\n",
      "loss: 0.54374146665\n",
      "loss: 0.574898593651\n",
      "loss: 0.568077131166\n",
      "loss: 0.570093237744\n",
      "loss: 0.568670185158\n",
      "loss: 0.568237014811\n",
      "iter 700 : accuracy =  690 / 1000 = 0.69\n",
      "loss: 0.578704283089\n",
      "loss: 0.567761434892\n",
      "loss: 0.52354671879\n",
      "loss: 0.582383664402\n",
      "loss: 0.56047898052\n",
      "loss: 0.547693602992\n",
      "loss: 0.571920748993\n",
      "loss: 0.585873609032\n",
      "loss: 0.574173260728\n",
      "loss: 0.552641689529\n",
      "loss: 0.571635018516\n",
      "loss: 0.567491492433\n",
      "loss: 0.567717294541\n",
      "loss: 0.57343478394\n",
      "loss: 0.55270373608\n",
      "loss: 0.556084405504\n",
      "loss: 0.550731210945\n",
      "loss: 0.575331362826\n",
      "loss: 0.581192084182\n",
      "loss: 0.588720042036\n",
      "loss: 0.550848450025\n",
      "loss: 0.592954939632\n",
      "loss: 0.581986360619\n",
      "loss: 0.549361330674\n",
      "loss: 0.561371498717\n",
      "loss: 0.543384535687\n",
      "loss: 0.563857521551\n",
      "loss: 0.600401204671\n",
      "loss: 0.540473685355\n",
      "loss: 0.56502353959\n",
      "loss: 0.562324975661\n",
      "loss: 0.576557491311\n",
      "loss: 0.589894225026\n",
      "loss: 0.572123774122\n",
      "loss: 0.579115874095\n",
      "loss: 0.577594498821\n",
      "loss: 0.54667313012\n",
      "loss: 0.564115679541\n",
      "loss: 0.527606774723\n",
      "loss: 0.585585014791\n",
      "loss: 0.538383444632\n",
      "loss: 0.556917939639\n",
      "loss: 0.553161815931\n",
      "loss: 0.537123418935\n",
      "loss: 0.583337311465\n",
      "loss: 0.569118532006\n",
      "loss: 0.539065061023\n",
      "loss: 0.542627698333\n",
      "loss: 0.576658083299\n",
      "loss: 0.556496872412\n",
      "loss: 0.570703525625\n",
      "loss: 0.54353387091\n",
      "loss: 0.58026269937\n",
      "loss: 0.582060912015\n",
      "loss: 0.546863543068\n",
      "loss: 0.539599639569\n",
      "loss: 0.57425544711\n",
      "loss: 0.568406493543\n",
      "loss: 0.559215131136\n",
      "loss: 0.560104575785\n",
      "loss: 0.549865019229\n",
      "loss: 0.546725164374\n",
      "loss: 0.571865596818\n",
      "loss: 0.564366501265\n",
      "loss: 0.545775588973\n",
      "loss: 0.587614865684\n",
      "loss: 0.548419984796\n",
      "loss: 0.560482099398\n",
      "loss: 0.540875476882\n",
      "loss: 0.574133216081\n",
      "loss: 0.590210268567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.547260699065\n",
      "loss: 0.571831274755\n",
      "loss: 0.548424677195\n",
      "loss: 0.545742479896\n",
      "loss: 0.563120629304\n",
      "loss: 0.563229536521\n",
      "loss: 0.590209814383\n",
      "loss: 0.583896217571\n",
      "loss: 0.56322769495\n",
      "loss: 0.577890529835\n",
      "loss: 0.579186508085\n",
      "loss: 0.570235404244\n",
      "loss: 0.550940521655\n",
      "loss: 0.569719304358\n",
      "loss: 0.523472085711\n",
      "loss: 0.571179065804\n",
      "loss: 0.553970550726\n",
      "loss: 0.567938084565\n",
      "loss: 0.580200089585\n",
      "loss: 0.582831329138\n",
      "loss: 0.544742039673\n",
      "loss: 0.58610505726\n",
      "loss: 0.597422060779\n",
      "loss: 0.542143563293\n",
      "loss: 0.573108529352\n",
      "loss: 0.566495647024\n",
      "loss: 0.568190894275\n",
      "loss: 0.56722861332\n",
      "loss: 0.566444736496\n",
      "iter 800 : accuracy =  690 / 1000 = 0.69\n",
      "loss: 0.577022954361\n",
      "loss: 0.564408311457\n",
      "loss: 0.523120023352\n",
      "loss: 0.579438938944\n",
      "loss: 0.559233985765\n",
      "loss: 0.546044604073\n",
      "loss: 0.570226804036\n",
      "loss: 0.584354211662\n",
      "loss: 0.572377496775\n",
      "loss: 0.551979507791\n",
      "loss: 0.568701215752\n",
      "loss: 0.566053560394\n",
      "loss: 0.565879890241\n",
      "loss: 0.570966243286\n",
      "loss: 0.551618008471\n",
      "loss: 0.553855253409\n",
      "loss: 0.549666297023\n",
      "loss: 0.573146442899\n",
      "loss: 0.579032962225\n",
      "loss: 0.586340923844\n",
      "loss: 0.549730748523\n",
      "loss: 0.590606753356\n",
      "loss: 0.581068193335\n",
      "loss: 0.547690636142\n",
      "loss: 0.558931994845\n",
      "loss: 0.542794458345\n",
      "loss: 0.561276089485\n",
      "loss: 0.597522845801\n",
      "loss: 0.538999828162\n",
      "loss: 0.563654114425\n",
      "loss: 0.561381954094\n",
      "loss: 0.57502308148\n",
      "loss: 0.587384397455\n",
      "loss: 0.569553368852\n",
      "loss: 0.57707313731\n",
      "loss: 0.575971947361\n",
      "loss: 0.544973581402\n",
      "loss: 0.562297773267\n",
      "loss: 0.526900937058\n",
      "loss: 0.584324945338\n",
      "loss: 0.537614511913\n",
      "loss: 0.555721599919\n",
      "loss: 0.551750288889\n",
      "loss: 0.536271681383\n",
      "loss: 0.581782321031\n",
      "loss: 0.567887761638\n",
      "loss: 0.537419610842\n",
      "loss: 0.541555692134\n",
      "loss: 0.575008678603\n",
      "loss: 0.554661496659\n",
      "loss: 0.568106159204\n",
      "loss: 0.54138248244\n",
      "loss: 0.578975978398\n",
      "loss: 0.58120413694\n",
      "loss: 0.546058967944\n",
      "loss: 0.537873868077\n",
      "loss: 0.572886707463\n",
      "loss: 0.56612215349\n",
      "loss: 0.558428233423\n",
      "loss: 0.558096270144\n",
      "loss: 0.548187519758\n",
      "loss: 0.546204260586\n",
      "loss: 0.570052463356\n",
      "loss: 0.563094446841\n",
      "loss: 0.544011923825\n",
      "loss: 0.585917948523\n",
      "loss: 0.546881673553\n",
      "loss: 0.558285465981\n",
      "loss: 0.540167315348\n",
      "loss: 0.573235893801\n",
      "loss: 0.589205718231\n",
      "loss: 0.546614094321\n",
      "loss: 0.569231803877\n",
      "loss: 0.546101582022\n",
      "loss: 0.545112843976\n",
      "loss: 0.561494077787\n",
      "loss: 0.561379247544\n",
      "loss: 0.589019443855\n",
      "loss: 0.582298195089\n",
      "loss: 0.562195328179\n",
      "loss: 0.576074055951\n",
      "loss: 0.577151194716\n",
      "loss: 0.568240711522\n",
      "loss: 0.549609082238\n",
      "loss: 0.568571362578\n",
      "loss: 0.521383121712\n",
      "loss: 0.568612543798\n",
      "loss: 0.551834582846\n",
      "loss: 0.566146340649\n",
      "loss: 0.578256168687\n",
      "loss: 0.58033453673\n",
      "loss: 0.542791386638\n",
      "loss: 0.583779988987\n",
      "loss: 0.59487490256\n",
      "loss: 0.540816822561\n",
      "loss: 0.571638787801\n",
      "loss: 0.565206897514\n",
      "loss: 0.566617852955\n",
      "loss: 0.566114385115\n",
      "loss: 0.564992258335\n",
      "iter 900 : accuracy =  690 / 1000 = 0.69\n",
      "loss: 0.575747928668\n",
      "loss: 0.561677450476\n",
      "loss: 0.522809398342\n",
      "loss: 0.577077181302\n",
      "loss: 0.558168120413\n",
      "loss: 0.544749885815\n",
      "loss: 0.568863909097\n",
      "loss: 0.583109042273\n",
      "loss: 0.570914916718\n",
      "loss: 0.551451043295\n",
      "loss: 0.566290985\n",
      "loss: 0.56487792582\n",
      "loss: 0.564358004981\n",
      "loss: 0.569069299445\n",
      "loss: 0.550867889781\n",
      "loss: 0.552105907794\n",
      "loss: 0.548848427903\n",
      "loss: 0.571374855302\n",
      "loss: 0.577240437743\n",
      "loss: 0.584439749782\n",
      "loss: 0.548835996541\n",
      "loss: 0.588698837121\n",
      "loss: 0.580383325099\n",
      "loss: 0.546285847156\n",
      "loss: 0.556888357804\n",
      "loss: 0.542334500775\n",
      "loss: 0.559181719584\n",
      "loss: 0.595180947256\n",
      "loss: 0.537794348337\n",
      "loss: 0.562546776109\n",
      "loss: 0.560619234597\n",
      "loss: 0.573855468306\n",
      "loss: 0.585300694426\n",
      "loss: 0.567465069636\n",
      "loss: 0.575483463272\n",
      "loss: 0.574717182615\n",
      "loss: 0.543593071578\n",
      "loss: 0.560754317787\n",
      "loss: 0.526340813075\n",
      "loss: 0.583309787896\n",
      "loss: 0.537020832924\n",
      "loss: 0.554757284124\n",
      "loss: 0.55058780217\n",
      "loss: 0.535583455408\n",
      "loss: 0.580548415074\n",
      "loss: 0.566952770633\n",
      "loss: 0.536107257449\n",
      "loss: 0.540696970572\n",
      "loss: 0.573648113284\n",
      "loss: 0.553175875454\n",
      "loss: 0.565974169323\n",
      "loss: 0.539644571658\n",
      "loss: 0.577968884813\n",
      "loss: 0.580579387284\n",
      "loss: 0.545565118087\n",
      "loss: 0.536500421284\n",
      "loss: 0.571814297587\n",
      "loss: 0.564240533038\n",
      "loss: 0.557811145152\n",
      "loss: 0.556540194119\n",
      "loss: 0.546826088762\n",
      "loss: 0.54578443298\n",
      "loss: 0.568601196275\n",
      "loss: 0.562043481062\n",
      "loss: 0.54257179288\n",
      "loss: 0.584596941304\n",
      "loss: 0.545628514005\n",
      "loss: 0.556465596691\n",
      "loss: 0.539615277479\n",
      "loss: 0.572572229842\n",
      "loss: 0.588440082173\n",
      "loss: 0.546157694653\n",
      "loss: 0.567102831547\n",
      "loss: 0.544272256135\n",
      "loss: 0.544684521893\n",
      "loss: 0.560211021595\n",
      "loss: 0.559915542959\n",
      "loss: 0.588100288745\n",
      "loss: 0.580979546737\n",
      "loss: 0.561353779643\n",
      "loss: 0.574610600353\n",
      "loss: 0.575476418575\n",
      "loss: 0.566621072028\n",
      "loss: 0.548555998884\n",
      "loss: 0.567635864014\n",
      "loss: 0.51965147212\n",
      "loss: 0.566492006449\n",
      "loss: 0.550125456091\n",
      "loss: 0.564725420641\n",
      "loss: 0.57665585983\n",
      "loss: 0.578288278889\n",
      "loss: 0.541206694399\n",
      "loss: 0.581903941337\n",
      "loss: 0.592881326834\n",
      "loss: 0.539721099229\n",
      "loss: 0.570435113417\n",
      "loss: 0.564165932314\n",
      "loss: 0.56532589307\n",
      "loss: 0.565262145216\n",
      "loss: 0.563822877349\n",
      "iter 1000 : accuracy =  690 / 1000 = 0.69\n",
      "iter cost: 0.399345055819 s in average.\n"
     ]
    }
   ],
   "source": [
    "w = LogisticRegression(train_data, tlabel,val_data, val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_data = []\n",
    "val_label = []\n",
    "for i in range(200000,201000):\n",
    "    temp1 = parseData(tdata[i])\n",
    "    temp2 = normalization(temp1)\n",
    "    val_data.append(temp2)\n",
    "    val_label.append(tlabel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MultiLR(tdata, tlabel, val_data, val_label, param = {'learning_rate':0.01, 'batch_size':100000, 'max_iter':10, 'n_jobs':10}):\n",
    "    #获取参数学习率，若无设置，默认为0.01\n",
    "    if param.has_key('learning_rate'):\n",
    "        learning_rate = param['learning_rate']\n",
    "    else:\n",
    "        learning_rate = 0.01\n",
    "    #获取参数batchsize，若无设置，默认为1000\n",
    "    if param.has_key('batch_size'):\n",
    "        batch = param['batch_size']\n",
    "    else:\n",
    "        batch = 1000\n",
    "    #获取参数最大迭代数，若无设置，默认为1000\n",
    "    if param.has_key('max_iter'):\n",
    "        max_iter = param['max_iter']\n",
    "    else:\n",
    "        max_iter = 1000\n",
    "    #获取进程数，若无设置，默认为10\n",
    "    if param.has_key('n_jobs'):\n",
    "        n_jobs = param['n_jobs']\n",
    "    else:\n",
    "        n_jobs = 2\n",
    "        \n",
    "    weights = [random.random() for i in range(202)]\n",
    "    num_of_jobs = batch / n_jobs\n",
    "    totalTime = 0.0\n",
    "    for iter in range(max_iter):\n",
    "        start = time.time()\n",
    "        lock = Lock()\n",
    "        rindex = int(0 + iter * batch)\n",
    "        while (rindex >= len(tdata)):\n",
    "            rindex = rindex - len(tdata)\n",
    "        error = Array('f', np.zeros(201))\n",
    "        loss = Value('f', 0.0)\n",
    "        #开启多进程\n",
    "        processes = []\n",
    "        for i in range(0, batch, num_of_jobs):\n",
    "            if i + num_of_jobs > batch:\n",
    "                continue\n",
    "            process = Process(target=cal_loss, \\\n",
    "                args=(weights, error, loss, tdata[rindex+i:rindex+i+num_of_jobs], tlabel[rindex+i:rindex+i+num_of_jobs], lock))\n",
    "            processes.append(process)\n",
    "        \n",
    "        #启动多进程\n",
    "        for i in range(len(processes)):\n",
    "            processes[i].start()\n",
    "            \n",
    "        #等待多进程结束\n",
    "        for i in range(len(processes)):\n",
    "            processes[i].join()\n",
    "            #print 'Process ', i, 'ended.'\n",
    "            \n",
    "        losses = loss.value / batch\n",
    "        print 'losses:',losses\n",
    "        for index in range(201):\n",
    "            weights[index + 1] = weights[index + 1] - learning_rate * (float(1)/batch) * error[index]\n",
    "        end = time.time()\n",
    "        totalTime = totalTime + float(end - start)\n",
    "        if ((iter + 1) % 10 == 0):\n",
    "            calAcc(weights, val_data, val_label, iter + 1)\n",
    "    print 'iter cost:', float(totalTime) / max_iter, 's in average.'\n",
    "    return weights\n",
    "\n",
    "def cal_loss(weights, error, loss, tdata, tlabel, lock):\n",
    "    losstemp = loss.value\n",
    "    error_temp = [error[i] for i in range(201)]\n",
    "        #print \"error_temp\",error_temp\n",
    "    for i in range(len(tdata)):\n",
    "        dataDict = tdata[i]\n",
    "        if (tlabel[i] == 10):\n",
    "            losstemp = losstemp - math.log(sigmoid(predict(weights, dataDict)))\n",
    "        else:\n",
    "            losstemp = losstemp - math.log(1 - sigmoid(predict(weights, dataDict))) \n",
    "        e_temp = sigmoid(predict(weights, dataDict)) - tlabel[i] \n",
    "        for index in dataDict:\n",
    "            error_temp[index - 1] = error_temp[index - 1] + e_temp * dataDict[index]\n",
    "    with lock:\n",
    "        loss.value += losstemp\n",
    "        for i in range(201):\n",
    "            error[i] += error_temp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses: 5.8211075\n",
      "losses: 5.538529375\n",
      "losses: 4.3342103125\n",
      "losses: 3.0888340625\n",
      "losses: 2.580300625\n",
      "losses: 1.98774203125\n",
      "losses: 1.90831296875\n",
      "losses: 1.80111078125\n",
      "losses: 1.757274375\n",
      "losses: 1.7442921875\n",
      "iter 10 : accuracy =  592 / 1000 = 0.592\n",
      "losses: 1.7469315625\n",
      "losses: 1.75758640625\n",
      "losses: 1.772221875\n",
      "losses: 1.78860953125\n",
      "losses: 1.80549421875\n",
      "losses: 1.82217078125\n",
      "losses: 1.83825265625\n",
      "losses: 1.73792640625\n",
      "losses: 1.867426875\n",
      "losses: 1.88084640625\n",
      "iter 20 : accuracy =  653 / 1000 = 0.653\n",
      "losses: 1.89343109375\n",
      "losses: 1.905191875\n",
      "losses: 1.916159375\n",
      "losses: 1.92637375\n",
      "losses: 1.93587984375\n",
      "losses: 1.9447228125\n",
      "losses: 1.9529471875\n",
      "losses: 1.96059453125\n",
      "losses: 1.9677046875\n",
      "losses: 1.97431484375\n",
      "iter 30 : accuracy =  667 / 1000 = 0.667\n",
      "losses: 1.98045828125\n",
      "losses: 1.986166875\n",
      "losses: 1.991469375\n",
      "losses: 1.9963925\n",
      "losses: 2.00096046875\n",
      "losses: 2.00519640625\n",
      "losses: 2.00912078125\n",
      "losses: 2.01275234375\n",
      "losses: 2.0161096875\n",
      "losses: 2.01920859375\n",
      "iter 40 : accuracy =  672 / 1000 = 0.672\n",
      "losses: 2.022064375\n",
      "losses: 2.02469109375\n",
      "losses: 2.0271015625\n",
      "losses: 2.029308125\n",
      "losses: 2.03132203125\n",
      "losses: 2.03315375\n",
      "losses: 2.034813125\n",
      "losses: 2.03630890625\n",
      "losses: 2.03765015625\n",
      "losses: 2.03884453125\n",
      "iter 50 : accuracy =  679 / 1000 = 0.679\n",
      "losses: 2.0398996875\n",
      "losses: 2.04082234375\n",
      "losses: 2.0416196875\n",
      "losses: 2.04229734375\n",
      "losses: 2.04286109375\n",
      "losses: 2.043316875\n",
      "losses: 2.04366984375\n",
      "losses: 2.0439246875\n",
      "losses: 2.0440859375\n",
      "losses: 2.04415796875\n",
      "iter 60 : accuracy =  687 / 1000 = 0.687\n",
      "losses: 2.0441453125\n",
      "losses: 2.0440515625\n",
      "losses: 1.91604265625\n",
      "losses: 2.04403078125\n",
      "losses: 2.04357921875\n",
      "losses: 2.0431196875\n",
      "losses: 2.04263\n",
      "losses: 2.04209796875\n",
      "losses: 2.04151875\n",
      "losses: 2.04089\n",
      "iter 70 : accuracy =  693 / 1000 = 0.693\n",
      "losses: 2.04021109375\n",
      "losses: 2.0394834375\n",
      "losses: 2.038708125\n",
      "losses: 2.037886875\n",
      "losses: 2.03702125\n",
      "losses: 2.03611359375\n",
      "losses: 2.0351653125\n",
      "losses: 2.03417828125\n",
      "losses: 1.905940625\n",
      "losses: 2.03253875\n",
      "iter 80 : accuracy =  693 / 1000 = 0.693\n",
      "losses: 2.03130796875\n",
      "losses: 2.03010375\n",
      "losses: 2.02890296875\n",
      "losses: 1.9008071875\n",
      "losses: 2.02691828125\n",
      "losses: 2.02553171875\n",
      "losses: 1.89750984375\n",
      "losses: 2.02330609375\n",
      "losses: 1.89529640625\n",
      "losses: 2.0208634375\n",
      "iter 90 : accuracy =  693 / 1000 = 0.693\n",
      "losses: 2.0193128125\n",
      "losses: 2.01782609375\n",
      "losses: 2.01636828125\n",
      "losses: 2.0149196875\n",
      "losses: 2.0134703125\n",
      "losses: 2.0120134375\n",
      "losses: 2.01054671875\n",
      "losses: 2.0090684375\n",
      "losses: 2.00757828125\n",
      "losses: 2.0060765625\n",
      "iter 100 : accuracy =  693 / 1000 = 0.693\n",
      "iter cost: 3.61713990688 s in average.\n"
     ]
    }
   ],
   "source": [
    "w1 = MultiLR(train_data, tlabel,val_data, val_label, param = {'learning_rate':0.01, 'batch_size':100000, 'max_iter':100, 'n_jobs':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.23651151833\n",
      "loss: 2.17199044125\n",
      "loss: 2.11114709575\n",
      "loss: 2.05251778177\n",
      "loss: 1.99528135642\n",
      "loss: 1.93897950488\n",
      "loss: 1.88336228751\n",
      "loss: 1.82830291174\n",
      "loss: 1.77375045862\n",
      "loss: 1.71970363535\n",
      "iter 10 : accuracy =  443 / 1000 = 0.443\n",
      "loss: 1.66619637689\n",
      "loss: 1.61329023199\n",
      "loss: 1.56107063795\n",
      "loss: 1.50964532115\n",
      "loss: 1.4591436293\n",
      "loss: 1.40971586544\n",
      "loss: 1.36153180415\n",
      "loss: 1.31477763885\n",
      "loss: 1.26965072604\n",
      "loss: 1.22635174257\n",
      "iter 20 : accuracy =  457 / 1000 = 0.457\n",
      "loss: 1.18507430828\n",
      "loss: 1.14599274163\n",
      "loss: 1.10924930175\n",
      "loss: 1.07494280905\n",
      "loss: 1.04312066365\n",
      "loss: 1.01377581462\n",
      "loss: 0.986849230155\n",
      "loss: 0.962237207554\n",
      "loss: 0.939801898539\n",
      "loss: 0.919383036107\n",
      "iter 30 : accuracy =  539 / 1000 = 0.539\n",
      "loss: 0.90080906432\n",
      "loss: 0.883906468571\n",
      "loss: 0.868506780433\n",
      "loss: 0.854451270692\n",
      "loss: 0.841593665537\n",
      "loss: 0.829801345803\n",
      "loss: 0.818955483201\n",
      "loss: 0.808950495491\n",
      "loss: 0.799693110741\n",
      "loss: 0.791101244663\n",
      "iter 40 : accuracy =  597 / 1000 = 0.597\n",
      "loss: 0.783102825038\n",
      "loss: 0.775634645262\n",
      "loss: 0.768641292901\n",
      "loss: 0.762074175268\n",
      "loss: 0.755890649113\n",
      "loss: 0.750053252672\n",
      "loss: 0.744529033525\n",
      "loss: 0.739288963465\n",
      "loss: 0.734307430803\n",
      "loss: 0.729561800743\n",
      "iter 50 : accuracy =  612 / 1000 = 0.612\n",
      "loss: 0.725032035044\n",
      "loss: 0.720700363036\n",
      "loss: 0.716550996952\n",
      "loss: 0.712569885435\n",
      "loss: 0.708744499859\n",
      "loss: 0.7050636489\n",
      "loss: 0.701517317372\n",
      "loss: 0.698096525963\n",
      "loss: 0.69479320898\n",
      "loss: 0.691600107606\n",
      "iter 60 : accuracy =  626 / 1000 = 0.626\n",
      "loss: 0.688510676576\n",
      "loss: 0.685519002429\n",
      "loss: 0.682619731824\n",
      "loss: 0.679808008544\n",
      "loss: 0.677079418086\n",
      "loss: 0.674429938832\n",
      "loss: 0.671855898969\n",
      "loss: 0.66935393842\n",
      "loss: 0.666920975179\n",
      "loss: 0.664554175482\n",
      "iter 70 : accuracy =  633 / 1000 = 0.633\n",
      "loss: 0.662250927371\n",
      "loss: 0.660008817218\n",
      "loss: 0.657825608886\n",
      "loss: 0.655699225189\n",
      "loss: 0.653627731407\n",
      "loss: 0.651609320611\n",
      "loss: 0.649642300592\n",
      "loss: 0.647725082222\n",
      "loss: 0.645856169082\n",
      "loss: 0.644034148222\n",
      "iter 80 : accuracy =  640 / 1000 = 0.64\n",
      "loss: 0.642257681929\n",
      "loss: 0.640525500395\n",
      "loss: 0.638836395187\n",
      "loss: 0.637189213437\n",
      "loss: 0.635582852673\n",
      "loss: 0.634016256222\n",
      "loss: 0.632488409129\n",
      "loss: 0.630998334532\n",
      "loss: 0.62954509045\n",
      "loss: 0.628127766934\n",
      "iter 90 : accuracy =  651 / 1000 = 0.651\n",
      "loss: 0.626745483545\n",
      "loss: 0.625397387129\n",
      "loss: 0.624082649841\n",
      "loss: 0.62280046741\n",
      "loss: 0.621550057604\n",
      "loss: 0.620330658873\n",
      "loss: 0.619141529163\n",
      "loss: 0.617981944859\n",
      "loss: 0.61685119986\n",
      "loss: 0.615748604767\n",
      "iter 100 : accuracy =  656 / 1000 = 0.656\n",
      "iter cost: 25.3353831482 s in average.\n"
     ]
    }
   ],
   "source": [
    "w = LogisticRegression(train_data, tlabel,val_data, val_label, param = {'learning_rate':0.01, 'batch_size':100000, 'max_iter':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
